{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot Text Classification Model\n",
    "**Model Description** - Bart with a classification head trained on MNLI.\n",
    "\n",
    "Sequences are posed as NLI premises and topic labels are turned into premises, i.e. business -> This text is about business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.6.0 available.\n",
      "TensorFlow version 2.3.0 available.\n"
     ]
    }
   ],
   "source": [
    "import eland as ed\n",
    "from eland.conftest import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import preprocessor as prep\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import BartForSequenceClassification, BartTokenizer\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data from Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_df = ed.DataFrame('localhost', 'twitter', columns=['full_text', 'user_id', 'verified', 'name', 'location', 'entities.hashtags.text', 'entities.user_mentions.name'])\n",
    "\n",
    "# defining the full-text query we need: Retrieving records for full_text_processed with the condition is_retweet=False and is_quote_status=False\n",
    "query_unique = {\n",
    "    \"bool\": {\n",
    "        \"must\": {\n",
    "            \"term\":{\"is_retweet\":\"false\"},\n",
    "        },\n",
    "        \"filter\": {\n",
    "            \"term\":{\"is_quote_status\":\"false\"},\n",
    "            \"term\":{\"lang.keyword\":\"en\"}\n",
    "        },\n",
    "    }\n",
    "}\n",
    "# using full-text search capabilities with Eland:\n",
    "df_ed = ed_df.es_query(query_unique)\n",
    "df_tweets = df_ed.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75523, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>verified</th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>entities.hashtags.text</th>\n",
       "      <th>entities.user_mentions.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1264253979002843136</th>\n",
       "      <td>The effect of Amphan in South 24 parganas http...</td>\n",
       "      <td>1256290967344168960</td>\n",
       "      <td>False</td>\n",
       "      <td>Avishek Pradhan</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264253893632016384</th>\n",
       "      <td>Dukkhor bishoye, onek khoti hoyeche. Hoping fo...</td>\n",
       "      <td>1249801865518145536</td>\n",
       "      <td>False</td>\n",
       "      <td>Aryan M.</td>\n",
       "      <td>Kolkata | Delhi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264253882580045824</th>\n",
       "      <td>Economic is in prblm, covid19 is arising it's ...</td>\n",
       "      <td>868107356</td>\n",
       "      <td>False</td>\n",
       "      <td>tk.sinha</td>\n",
       "      <td>Kolkata, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264253658763612160</th>\n",
       "      <td>@DrSJaishankar @MEAIndia How many days more In...</td>\n",
       "      <td>130462260</td>\n",
       "      <td>False</td>\n",
       "      <td>Mohammad Khalilullah</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Dr. S. Jaishankar, Anurag Srivastava]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264253569525592064</th>\n",
       "      <td>@HCI_London @MEAIndia How many days more India...</td>\n",
       "      <td>130462260</td>\n",
       "      <td>False</td>\n",
       "      <td>Mohammad Khalilullah</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[India in the UK, Anurag Srivastava]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             full_text              user_id  \\\n",
       "1264253979002843136  The effect of Amphan in South 24 parganas http...  1256290967344168960   \n",
       "1264253893632016384  Dukkhor bishoye, onek khoti hoyeche. Hoping fo...  1249801865518145536   \n",
       "1264253882580045824  Economic is in prblm, covid19 is arising it's ...            868107356   \n",
       "1264253658763612160  @DrSJaishankar @MEAIndia How many days more In...            130462260   \n",
       "1264253569525592064  @HCI_London @MEAIndia How many days more India...            130462260   \n",
       "\n",
       "                     verified                  name         location entities.hashtags.text  \\\n",
       "1264253979002843136     False       Avishek Pradhan      West Bengal                    NaN   \n",
       "1264253893632016384     False              Aryan M.  Kolkata | Delhi                    NaN   \n",
       "1264253882580045824     False              tk.sinha   Kolkata, India                    NaN   \n",
       "1264253658763612160     False  Mohammad Khalilullah             None                    NaN   \n",
       "1264253569525592064     False  Mohammad Khalilullah             None                    NaN   \n",
       "\n",
       "                                entities.user_mentions.name  \n",
       "1264253979002843136                                     NaN  \n",
       "1264253893632016384                                     NaN  \n",
       "1264253882580045824                                     NaN  \n",
       "1264253658763612160  [Dr. S. Jaishankar, Anurag Srivastava]  \n",
       "1264253569525592064    [India in the UK, Anurag Srivastava]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tweet Preprocessing\n",
    "- Remove URLs and reserved words (RTs)\n",
    "- Remove # and @ symbols\n",
    "- Remove tweets less than 4 tokens in length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set options for the tweet-preprocessor\n",
    "prep.set_options(prep.OPT.URL, prep.OPT.RESERVED, prep.OPT.EMOJI, prep.OPT.SMILEY)\n",
    "\n",
    "## Clean text and remove #,@ symbols\n",
    "def clean_tweet(text):\n",
    "    text = prep.clean(text)\n",
    "    table = str.maketrans('','','#@')\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['full_text'] = df_tweets['full_text'].apply(lambda x: clean_tweet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['length'] = df_tweets['full_text'].apply(lambda x: len([w for w in x.split()]))\n",
    "df_tweets = df_tweets[df_tweets['length']>4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>verified</th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>entities.hashtags.text</th>\n",
       "      <th>entities.user_mentions.name</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1264253979002843136</th>\n",
       "      <td>The effect of Amphan in South 24 parganas</td>\n",
       "      <td>1256290967344168960</td>\n",
       "      <td>False</td>\n",
       "      <td>Avishek Pradhan</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264253893632016384</th>\n",
       "      <td>Dukkhor bishoye, onek khoti hoyeche. Hoping fo...</td>\n",
       "      <td>1249801865518145536</td>\n",
       "      <td>False</td>\n",
       "      <td>Aryan M.</td>\n",
       "      <td>Kolkata | Delhi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264253882580045824</th>\n",
       "      <td>Economic is in prblm, covid19 is arising it's ...</td>\n",
       "      <td>868107356</td>\n",
       "      <td>False</td>\n",
       "      <td>tk.sinha</td>\n",
       "      <td>Kolkata, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264253658763612160</th>\n",
       "      <td>DrSJaishankar MEAIndia How many days more Indi...</td>\n",
       "      <td>130462260</td>\n",
       "      <td>False</td>\n",
       "      <td>Mohammad Khalilullah</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Dr. S. Jaishankar, Anurag Srivastava]</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264253569525592064</th>\n",
       "      <td>HCI_London MEAIndia How many days more Indian ...</td>\n",
       "      <td>130462260</td>\n",
       "      <td>False</td>\n",
       "      <td>Mohammad Khalilullah</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[India in the UK, Anurag Srivastava]</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             full_text              user_id  \\\n",
       "1264253979002843136          The effect of Amphan in South 24 parganas  1256290967344168960   \n",
       "1264253893632016384  Dukkhor bishoye, onek khoti hoyeche. Hoping fo...  1249801865518145536   \n",
       "1264253882580045824  Economic is in prblm, covid19 is arising it's ...            868107356   \n",
       "1264253658763612160  DrSJaishankar MEAIndia How many days more Indi...            130462260   \n",
       "1264253569525592064  HCI_London MEAIndia How many days more Indian ...            130462260   \n",
       "\n",
       "                     verified                  name         location entities.hashtags.text  \\\n",
       "1264253979002843136     False       Avishek Pradhan      West Bengal                    NaN   \n",
       "1264253893632016384     False              Aryan M.  Kolkata | Delhi                    NaN   \n",
       "1264253882580045824     False              tk.sinha   Kolkata, India                    NaN   \n",
       "1264253658763612160     False  Mohammad Khalilullah             None                    NaN   \n",
       "1264253569525592064     False  Mohammad Khalilullah             None                    NaN   \n",
       "\n",
       "                                entities.user_mentions.name  length  \n",
       "1264253979002843136                                     NaN       8  \n",
       "1264253893632016384                                     NaN      37  \n",
       "1264253882580045824                                     NaN      22  \n",
       "1264253658763612160  [Dr. S. Jaishankar, Anurag Srivastava]      48  \n",
       "1264253569525592064    [India in the UK, Anurag Srivastava]      48  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/ubuntu/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/ubuntu/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-mnli/config.json from cache at /home/ubuntu/.cache/torch/transformers/a35b79dc26c2f371a0e19eae44d91c0a0281a5db09044517d2675703791ee3c5.746d7ef19ade685cd3ee03f131a96fab513947c26179546289ddf02a6ac683ce\n",
      "Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"entailment\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 2,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://cdn.huggingface.co/facebook/bart-large-mnli/pytorch_model.bin from cache at /home/ubuntu/.cache/torch/transformers/c95cb3cea0a948fd7153bdc1619ccb5af6aecfe81c1c9bc8bc68e0f996ea0e99.eeaeadf372d867602df9d8df9899a1ee0ab4210002c7c1d91f3e10b6e852657b\n",
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BartForSequenceClassification were initialized from the model checkpoint at facebook/bart-large-mnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "model = BartForSequenceClassification.from_pretrained('facebook/bart-large-mnli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example on how to use the Model\n",
    "The sequence (text) is posed as a NLI Premise and the label as a hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that the label is true: 98.08%\n"
     ]
    }
   ],
   "source": [
    "# pose sequence as a NLI premise and label (politics) as a hypothesis\n",
    "premise = 'Who are you voting for in 2020?'\n",
    "hypothesis = 'This text is about politics.'\n",
    "\n",
    "# run through model pre-trained on MNLI\n",
    "input_ids = tokenizer.encode(premise, hypothesis, return_tensors='pt')\n",
    "logits = model(input_ids)[0]\n",
    "\n",
    "# we throw away \"neutral\" (dim 1) and take the probability of\n",
    "# \"entailment\" (2) as the probability of the label being true \n",
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "probs = entail_contradiction_logits.softmax(dim=1)\n",
    "true_prob = probs[:,1].item() * 100\n",
    "print(f'Probability that the label is true: {true_prob:0.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Labels and Threshold for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TERMS = ['resource availability', 'volunteers', 'power supply', 'relief measures', 'food supply', 'infrastructure', 'medical assistance', 'rescue', 'shelter', 'utilities', 'water supply', 'evacuation', 'government', 'crime violence', 'mobile network', 'sympathy', 'news updates', 'internet', 'grievance']\n",
    "\n",
    "HYPOTHESES = ['This text is about '+x for x in TERMS]\n",
    "THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Method to get the labels for a tweet based on threshold specified'''\n",
    "def get_labels(premise, threshold=THRESHOLD):\n",
    "    topics = []\n",
    "    for idx, hypothesis in enumerate(HYPOTHESES):\n",
    "        # run through model pre-trained on MNLI\n",
    "        input_ids = tokenizer.encode(premise, hypothesis, return_tensors='pt')\n",
    "        logits = model(input_ids)[0]\n",
    "\n",
    "        # we throw away \"neutral\" (dim 1) and take the probability of\n",
    "        # \"entailment\" (2) as the probability of the label being true \n",
    "        entail_contradiction_logits = logits[:,[0,2]]\n",
    "        probs = entail_contradiction_logits.softmax(dim=1)\n",
    "        true_prob = probs[:,1].item() * 100\n",
    "\n",
    "        if true_prob>=threshold:\n",
    "            topics.append((TERMS[idx], np.round(true_prob,2)))\n",
    "    \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "df_tweets['labels'] = df_tweets['full_text'].progress_apply(lambda x: get_labels(x, THRESHOLD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU approach using Transformers Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-mnli/config.json from cache at /home/ubuntu/.cache/torch/transformers/a35b79dc26c2f371a0e19eae44d91c0a0281a5db09044517d2675703791ee3c5.746d7ef19ade685cd3ee03f131a96fab513947c26179546289ddf02a6ac683ce\n",
      "Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"entailment\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 2,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/ubuntu/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/ubuntu/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "Model card: {\n",
      "  \"caveats_and_recommendations\": {},\n",
      "  \"ethical_considerations\": {},\n",
      "  \"evaluation_data\": {},\n",
      "  \"factors\": {},\n",
      "  \"intended_use\": {},\n",
      "  \"metrics\": {},\n",
      "  \"model_details\": {},\n",
      "  \"quantitative_analyses\": {},\n",
      "  \"training_data\": {}\n",
      "}\n",
      "\n",
      "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-mnli/config.json from cache at /home/ubuntu/.cache/torch/transformers/a35b79dc26c2f371a0e19eae44d91c0a0281a5db09044517d2675703791ee3c5.746d7ef19ade685cd3ee03f131a96fab513947c26179546289ddf02a6ac683ce\n",
      "Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"entailment\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 2,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://cdn.huggingface.co/facebook/bart-large-mnli/pytorch_model.bin from cache at /home/ubuntu/.cache/torch/transformers/c95cb3cea0a948fd7153bdc1619ccb5af6aecfe81c1c9bc8bc68e0f996ea0e99.eeaeadf372d867602df9d8df9899a1ee0ab4210002c7c1d91f3e10b6e852657b\n",
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BartForSequenceClassification were initialized from the model checkpoint at facebook/bart-large-mnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('zero-shot-classification', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TERMS = ['resource availability', 'volunteers', 'power supply', 'relief measures', \n",
    "         'food supply', 'infrastructure', 'medical assistance', 'rescue', 'shelter', \n",
    "         'utilities', 'water supply', 'evacuation', 'government', 'crime violence', \n",
    "         'mobile network', 'sympathy', 'news updates', 'internet', 'grievance', \n",
    "         'livelihood', 'income', 'ecosystem', 'biodiversity', 'agriculture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.4 ms ± 946 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit classifier(df_tweets['full_text'][0], TERMS, multi_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Method to get the labels for a tweet based on threshold specified'''\n",
    "def get_all_labels(x, terms=TERMS):\n",
    "    \n",
    "    # Run model\n",
    "    result = classifier(x, terms, multi_class=True)\n",
    "    \n",
    "    topics = []\n",
    "    for label, score in zip(result['labels'], result['scores']):\n",
    "\n",
    "        topics.append((label, np.round(score,2)))\n",
    "            \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1c60f9a2b0421aa16b534f8f8a4903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=74289.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df_tweets['full_text'].progress_apply(lambda x: get_all_labels(x, TERMS)).to_json('../models/zstc_labels.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
