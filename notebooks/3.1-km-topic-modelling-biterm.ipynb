{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597361890685",
   "display_name": "Python 3.8.5 64-bit ('solve-iwmi': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling using Biterm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eland as ed\n",
    "from eland.conftest import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora, models\n",
    "from wordcloud import WordCloud\n",
    "from biterm.utility import vec_to_biterms\n",
    "from biterm.btm import oBTM\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "from pprint import pprint\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data from Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_df = ed.DataFrame('localhost', 'twitter', columns=['full_text_processed', 'user_id', 'verified', 'name', 'location', 'entities.hashtags.text', 'entities.user_mentions.name'])\n",
    "\n",
    "# defining the full-text query we need: Retrieving records for full_text_processed with the condition is_retweet=False and is_quote_status=False\n",
    "query_unique = {\n",
    "    \"bool\": {\n",
    "        \"must\": {\n",
    "            \"term\":{\"is_retweet\":\"false\"},\n",
    "        },\n",
    "        \"filter\": {\n",
    "            \"term\":{\"is_quote_status\":\"false\"}\n",
    "        },\n",
    "    }\n",
    "}\n",
    "# using full-text search capabilities with Eland:\n",
    "df_ed = ed_df.es_query(query_unique)\n",
    "df_tweets = df_ed.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   full_text_processed              user_id  \\\n1264160647002103808  praying everyone affected condolence family vi...  1256622599364214786   \n1264160609668599808  cyclone ampan people satkhira upset due lack w...  1251934220345208832   \n1264121161589415936  cyclone amphan ha completely destroyed agricul...  1251934220345208832   \n1264160569315209216  amphan cyclone ​​cm mamta demand ban labor spe...  1113075640499036160   \n1264114187346874368  amfan storm caused devastation bengal mp nusra...  1113075640499036160   \n\n                     verified         name               location entities.hashtags.text  \\\n1264160647002103808     False  The Meraaki  Ahmadabad City, India     AmphanSuperCyclone   \n1264160609668599808     False   Newspapers                  Dhaka                    NaN   \n1264121161589415936     False   Newspapers                  Dhaka                    NaN   \n1264160569315209216     False      netvani                   None                    NaN   \n1264114187346874368     False      netvani                   None                    NaN   \n\n                    entities.user_mentions.name  \n1264160647002103808                         NaN  \n1264160609668599808                         NaN  \n1264121161589415936                         NaN  \n1264160569315209216                         NaN  \n1264114187346874368                         NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full_text_processed</th>\n      <th>user_id</th>\n      <th>verified</th>\n      <th>name</th>\n      <th>location</th>\n      <th>entities.hashtags.text</th>\n      <th>entities.user_mentions.name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1264160647002103808</th>\n      <td>praying everyone affected condolence family vi...</td>\n      <td>1256622599364214786</td>\n      <td>False</td>\n      <td>The Meraaki</td>\n      <td>Ahmadabad City, India</td>\n      <td>AmphanSuperCyclone</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1264160609668599808</th>\n      <td>cyclone ampan people satkhira upset due lack w...</td>\n      <td>1251934220345208832</td>\n      <td>False</td>\n      <td>Newspapers</td>\n      <td>Dhaka</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1264121161589415936</th>\n      <td>cyclone amphan ha completely destroyed agricul...</td>\n      <td>1251934220345208832</td>\n      <td>False</td>\n      <td>Newspapers</td>\n      <td>Dhaka</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1264160569315209216</th>\n      <td>amphan cyclone ​​cm mamta demand ban labor spe...</td>\n      <td>1113075640499036160</td>\n      <td>False</td>\n      <td>netvani</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1264114187346874368</th>\n      <td>amfan storm caused devastation bengal mp nusra...</td>\n      <td>1113075640499036160</td>\n      <td>False</td>\n      <td>netvani</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenising and removing short tweets (less than 4 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['full_text_processed'] = df_tweets['full_text_processed'].apply(lambda x: remove_emoji(x))\n",
    "df_tweets['full_text_tokens'] = df_tweets['full_text_processed'].apply(lambda x: [w for w in x.split()])\n",
    "df_tweets['length'] = df_tweets['full_text_tokens'].apply(lambda x: len(x))\n",
    "df_tweets = df_tweets[df_tweets['length']>4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from','not', 'would', 'say', 'could', '_', 'be', 'go', 'do', 'rather', 'seem', 'due', 'via', 'done', 'said'])\n",
    "\n",
    "tweets = df_tweets.full_text_processed.to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,4), stop_words=stop_words, min_df=5, token_pattern='(?u)\\\\b[^\\\\s|\\\\.]+\\\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(95337, 79597)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Create the TF-IDF matrix\n",
    "tweets_tfidf = vectorizer.fit_transform(tweets)\n",
    "tweets_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents according to the parameters in the vectorizer\n",
    "for i in range(len(tweets)):\n",
    "    tweets[i] = vectorizer.build_analyzer()(tweets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary and corpus for Gensim to compute coherence metrics\n",
    "dictionary=corpora.Dictionary(tweets)\n",
    "corpus=[dictionary.doc2bow(doc) for doc in tweets]\n",
    "# Adding the TF-IDF for better insight \n",
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the vocabulary and the biterms from the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "biterms = vec_to_biterms(tweets_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create btm\n",
    "btm = oBTM(num_topics=10, V=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n Train BTM ..\")\n",
    "for i in range(0, len(biterms), 10000): # process chunkwise\n",
    "    biterms_chunk = biterms[i:i + 10000]\n",
    "    btm.fit(biterms_chunk, iterations=1)\n",
    "    topics = btm.transform(biterms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}