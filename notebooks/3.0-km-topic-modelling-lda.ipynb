{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597275963205",
   "display_name": "Python 3.8.5 64-bit ('solve-iwmi': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling using LDA (Latent  Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import eland as ed\n",
    "from eland.conftest import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "from pprint import pprint\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel, Phrases\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data from Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_df = ed.DataFrame('localhost', 'twitter', columns=['full_text_processed'])\n",
    "\n",
    "# defining the full-text query we need: Retrieving records for full_text_processed with the condition is_retweet=False and is_quote_status=False\n",
    "query_unique = {\n",
    "    \"bool\": {\n",
    "        \"must\": {\n",
    "            \"term\":{\"is_retweet\":\"false\"},\n",
    "        },\n",
    "        \"filter\": {\n",
    "            \"term\":{\"is_quote_status\":\"false\"}\n",
    "        },\n",
    "    }\n",
    "}\n",
    "# using full-text search capabilities with Eland:\n",
    "df_ed = ed_df.es_query(query_unique)\n",
    "df_tweets = df_ed.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   full_text_processed\n1262754556456271872  proactive plan pm narendra modi take stock cyc...\n1262754552505184256                 odisha bengal ha amphan last 2 day\n1262754542589861888            many test given human race corona ampan\n1262754528979345408  latest release indian meteorological departmen...\n1262746620736598016  visit principal secretary govtcommerce amptran...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full_text_processed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1262754556456271872</th>\n      <td>proactive plan pm narendra modi take stock cyc...</td>\n    </tr>\n    <tr>\n      <th>1262754552505184256</th>\n      <td>odisha bengal ha amphan last 2 day</td>\n    </tr>\n    <tr>\n      <th>1262754542589861888</th>\n      <td>many test given human race corona ampan</td>\n    </tr>\n    <tr>\n      <th>1262754528979345408</th>\n      <td>latest release indian meteorological departmen...</td>\n    </tr>\n    <tr>\n      <th>1262746620736598016</th>\n      <td>visit principal secretary govtcommerce amptran...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising and removing short tweets (less than 4 words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['full_text_processed'] = df_tweets['full_text_processed'].apply(lambda x: remove_emoji(x))\n",
    "df_tweets['full_text_tokens'] = df_tweets['full_text_processed'].apply(lambda x: [w for w in x.split()])\n",
    "df_tweets['length'] = df_tweets['full_text_tokens'].apply(lambda x: len(x))\n",
    "df_tweets = df_tweets[df_tweets['length']>4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   full_text_processed  \\\n1262754556456271872  proactive plan pm narendra modi take stock cyc...   \n1262754552505184256                 odisha bengal ha amphan last 2 day   \n1262754542589861888            many test given human race corona ampan   \n1262754528979345408  latest release indian meteorological departmen...   \n1262746620736598016  visit principal secretary govtcommerce amptran...   \n\n                                                      full_text_tokens  length  \n1262754556456271872  [proactive, plan, pm, narendra, modi, take, st...      15  \n1262754552505184256         [odisha, bengal, ha, amphan, last, 2, day]       7  \n1262754542589861888    [many, test, given, human, race, corona, ampan]       7  \n1262754528979345408  [latest, release, indian, meteorological, depa...      12  \n1262746620736598016  [visit, principal, secretary, govtcommerce, am...      18  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full_text_processed</th>\n      <th>full_text_tokens</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1262754556456271872</th>\n      <td>proactive plan pm narendra modi take stock cyc...</td>\n      <td>[proactive, plan, pm, narendra, modi, take, st...</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>1262754552505184256</th>\n      <td>odisha bengal ha amphan last 2 day</td>\n      <td>[odisha, bengal, ha, amphan, last, 2, day]</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1262754542589861888</th>\n      <td>many test given human race corona ampan</td>\n      <td>[many, test, given, human, race, corona, ampan]</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1262754528979345408</th>\n      <td>latest release indian meteorological departmen...</td>\n      <td>[latest, release, indian, meteorological, depa...</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>1262746620736598016</th>\n      <td>visit principal secretary govtcommerce amptran...</td>\n      <td>[visit, principal, secretary, govtcommerce, am...</td>\n      <td>18</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Bigram and Trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from','not', 'would', 'say', 'could', '_', 'be', 'go', 'do', 'rather', 'seem', 'due', 'via', 'done', 'said'])\n",
    "\n",
    "tweets_list = df_tweets.full_text_tokens.to_list()\n",
    "tweet_ids = df_tweets.index.to_list()\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "\n",
    "bigram = Phrases(tweets_list, min_count=10, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = Phrases(bigram[tweets_list], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [[word for word in gensim.utils.simple_preprocess(str(tweet))] for tweet in tweets_list]\n",
    "tweets = [bigram_mod[tweet] for tweet in tweets]\n",
    "tweets = [trigram_mod[bigram_mod[tweet]] for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "tweets_dict = corpora.Dictionary(tweets)\n",
    "\n",
    "# Filtering extremes by removing tokens occuring in less than 10 tweets and have occured in more than 90% tweets\n",
    "tweets_dict.filter_extremes(no_below=10, no_above=0.9)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [tweets_dict.doc2bow(twt) for twt in tweets]\n",
    "\n",
    "# Adding the TF-IDF for better insight \n",
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on Hyperparameter optimization - trying 2 approaches:\n",
    "- Topics = 6, Alpha = 0.01\n",
    "- Topics = 10, Alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Model Parameters\n",
    "\n",
    "NUM_TOPICS_1 = 10\n",
    "ALPHA_1 = 1\n",
    "NUM_TOPICS_2 = 6\n",
    "ALPHA_2 = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model_build(corpus, dictionary, topics, alpha, texts):\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=dictionary,\n",
    "                                            num_topics=topics, \n",
    "                                            random_state=100,\n",
    "                                            passes=10,\n",
    "                                            alpha=alpha,\n",
    "                                            per_word_topics=True)\n",
    "    \n",
    "    print(\"\\nModel, Topics=\",topics)\n",
    "    pprint(lda_model.print_topics())\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    return lda_model, coherence_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nModel, Topics= 10\n[(0,\n  '0.068*\"bengal\" + 0.065*\"west\" + 0.040*\"pm\" + 0.040*\"modi\" + 0.032*\"odisha\" '\n  '+ 0.028*\"relief\" + 0.027*\"crore\" + 0.017*\"lakh\" + 0.016*\"say\" + '\n  '0.016*\"visit\"'),\n (1,\n  '0.025*\"nisarga\" + 0.016*\"like\" + 0.015*\"mumbai\" + 0.014*\"nisarg\" + '\n  '0.012*\"come\" + 0.011*\"earthquake\" + 0.010*\"nature\" + 0.010*\"much\" + '\n  '0.009*\"know\" + 0.009*\"hope\"'),\n (2,\n  '0.050*\"storm\" + 0.045*\"hit\" + 0.038*\"super\" + 0.028*\"may\" + 0.024*\"coast\" + '\n  '0.021*\"wind\" + 0.015*\"hour\" + 0.014*\"speed\" + 0.012*\"cyclonic\" + '\n  '0.012*\"strong\"'),\n (3,\n  '0.083*\"india\" + 0.063*\"bangladesh\" + 0.042*\"cyclone\" + 0.033*\"news\" + '\n  '0.033*\"via\" + 0.031*\"amphan\" + 0.028*\"kolkata\" + 0.025*\"dead\" + '\n  '0.024*\"million\" + 0.020*\"least\"'),\n (4,\n  '0.026*\"make\" + 0.025*\"landfall\" + 0.024*\"time\" + 0.024*\"hurricane\" + '\n  '0.023*\"today\" + 0.021*\"killed\" + 0.019*\"many\" + 0.018*\"tree\" + '\n  '0.017*\"house\" + 0.016*\"two\"'),\n (5,\n  '0.022*\"day\" + 0.017*\"please\" + 0.013*\"still\" + 0.012*\"get\" + 0.011*\"water\" '\n  '+ 0.011*\"even\" + 0.011*\"power\" + 0.011*\"service\" + 0.010*\"since\" + '\n  '0.010*\"electricity\"'),\n (6,\n  '0.035*\"wa\" + 0.020*\"life\" + 0.020*\"help\" + 0.018*\"ampan\" + 0.016*\"রনবন\" + '\n  '0.014*\"family\" + 0.014*\"ৰম\" + 0.012*\"victim\" + 0.012*\"new\" + '\n  '0.011*\"rebuild\"'),\n (7,\n  '0.060*\"people\" + 0.050*\"ha\" + 0.034*\"amp\" + 0.024*\"covid\" + '\n  '0.017*\"coronavirus\" + 0.017*\"home\" + 0.016*\"country\" + 0.015*\"pandemic\" + '\n  '0.015*\"crisis\" + 0.014*\"time\"'),\n (8,\n  '0.050*\"affected\" + 0.049*\"amfan\" + 0.040*\"due\" + 0.037*\"area\" + '\n  '0.028*\"cyclone\" + 0.022*\"odisha\" + 0.018*\"take\" + 0.017*\"situation\" + '\n  '0.017*\"tomorrow\" + 0.016*\"amphan\"'),\n (9,\n  '0.036*\"state\" + 0.031*\"damage\" + 0.027*\"government\" + 0.026*\"caused\" + '\n  '0.025*\"bjp\" + 0.025*\"corona\" + 0.023*\"disaster\" + 0.023*\"devastation\" + '\n  '0.022*\"year\" + 0.017*\"see\"')]\n"
    }
   ],
   "source": [
    "# Build first model - Topics=10\n",
    "lda_model_1, score_1 = lda_model_build(corpus=tfidf_corpus, dictionary=tweets_dict, topics=NUM_TOPICS_1, alpha=ALPHA_1, texts=tweets)\n",
    "\n",
    "# Build first model - Topics=6\n",
    "lda_model_2, score_2 = lda_model_build(corpus=tfidf_corpus, dictionary=tweets_dict, topics=NUM_TOPICS_2, alpha=ALPHA_2, texts=tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model 1 - Topics = 10, Score = 0.46908414489685174\n"
    }
   ],
   "source": [
    "## Coherence Scores\n",
    "\n",
    "print(\"Model 1 - Topics = 10, Score =\",score_1)\n",
    "print(\"Model 2 - Topics = 6, Score =\",score_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further analysis on Model 1 (Topics = 10)\n",
    "Addressing certain questions and extracting more information out of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Dominant Topic for each tweet and its percentage contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dominant_topic(lda_model, corpus):\n",
    "    tweet_topics = []\n",
    "    tweet_topics_percent = []\n",
    "    for tweet in tfidf_corpus:\n",
    "        topics_dist = lda_model.get_document_topics(tweet)\n",
    "        dom_topic, percent = max(topics_dist, key=lambda item:item[1])\n",
    "        tweet_topics.append(dom_topic)\n",
    "        tweet_topics_percent.append(percent)\n",
    "    return tweet_topics, tweet_topics_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_topics, tweet_topics_percent = get_dominant_topic(lda_model_1, tfidf_corpus) ## Storing the topic assignments for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_topics_df = pd.DataFrame(list(zip(tweets, tweet_topics, tweet_topics_percent)), columns=['Tokenized Tweet', 'Topic', 'Percentage Contribution'], index=tweet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                       Tokenized Tweet  Topic  \\\n1262754556456271872  [proactive, plan, pm, narendra, modi, take_sto...      4   \n1262754552505184256            [odisha, bengal, ha, amphan, last, day]      4   \n1262754542589861888     [many, test, given, human_race, corona, ampan]      6   \n1262754528979345408  [latest, release, indian, meteorological_depar...      6   \n1262746620736598016  [visit, principal_secretary, govtcommerce, amp...      4   \n\n                     Percentage Contribution  \n1262754556456271872                 0.175766  \n1262754552505184256                 0.137140  \n1262754542589861888                 0.190081  \n1262754528979345408                 0.141426  \n1262746620736598016                 0.145735  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tokenized Tweet</th>\n      <th>Topic</th>\n      <th>Percentage Contribution</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1262754556456271872</th>\n      <td>[proactive, plan, pm, narendra, modi, take_sto...</td>\n      <td>4</td>\n      <td>0.175766</td>\n    </tr>\n    <tr>\n      <th>1262754552505184256</th>\n      <td>[odisha, bengal, ha, amphan, last, day]</td>\n      <td>4</td>\n      <td>0.137140</td>\n    </tr>\n    <tr>\n      <th>1262754542589861888</th>\n      <td>[many, test, given, human_race, corona, ampan]</td>\n      <td>6</td>\n      <td>0.190081</td>\n    </tr>\n    <tr>\n      <th>1262754528979345408</th>\n      <td>[latest, release, indian, meteorological_depar...</td>\n      <td>6</td>\n      <td>0.141426</td>\n    </tr>\n    <tr>\n      <th>1262746620736598016</th>\n      <td>[visit, principal_secretary, govtcommerce, amp...</td>\n      <td>4</td>\n      <td>0.145735</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "tweet_topics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Tweets for Each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Topic\n0    10112\n1    12243\n2    11862\n3    10066\n4     7967\n5    11603\n6     8683\n7     8207\n8     7198\n9     7396\nName: Tokenized Tweet, dtype: int64"
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "tweet_topics_df.groupby('Topic')['Tokenized Tweet'].agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared_1 = pyLDAvis.gensim.prepare(lda_model_1, tfidf_corpus, tweets_dict)\n",
    "LDAvis_prepared_2 = pyLDAvis.gensim.prepare(lda_model_2, tfidf_corpus, tweets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the HTML\n",
    "pyLDAvis.save_html(LDAvis_prepared_1, '../reports/figures/LDA_topic_10.html')\n",
    "pyLDAvis.save_html(LDAvis_prepared_2, '../reports/figures/LDA_topic_6.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}